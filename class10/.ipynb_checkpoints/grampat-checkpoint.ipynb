{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import numpy\n",
    "maxDegree = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgPreps = 'in_favor_of|_|about|after|against|among|as|at|between|behind|by|for|from|in|into|of|on|upon|over|through|to|toward|towarV in favour of\truled in favour ofV in favour of\truled in favour ofds|with'.split('|')\n",
    "otherPreps ='out|down'.split('|')\n",
    "verbpat = ('V; V n; V ord; V oneself; V adj; V -ing; V to v; V v; V that; V wh; V wh to v; V quote; '+\\\n",
    "              'V so; V not; V as if; V as though; V someway; V together; V as adj; V as to wh; V by amount; '+\\\n",
    "              'V amount; V by -ing; V in favour of n; V in favour of ing; V n in favour of n; V n in favour of ing; V n n; V n adj; V n -ing; V n to v; V n v n; V n that; '+\\\n",
    "              'V n wh; V n wh to v; V n quote; V n v-ed; V n someway; V n with together; '+\\\n",
    "              'V n as adj; V n into -ing; V adv; V and v').split('; ')\n",
    "verbpat += ['V %s n' % prep for prep in pgPreps]+['V n %s n' % prep for prep in verbpat]\n",
    "verbpat += [pat.replace('V ', 'V-ed ') for pat in verbpat]\n",
    "reservedWords = 'how wh; who wh; what wh; when wh; someway someway; together together; that that'.split('; ')\n",
    "pronOBJ = ['me', 'us', 'you', 'him', 'them']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgNoun = ('N for n to v; N from n that; N from n to v; N from n for n; N in favor of; N in favour of; '+\\\n",
    "            'N of amount; N of n as n; N of n to n; N of n with n; N on n for n; N on n to v'+\\\n",
    "            'N that; N to v; N to n that; N to n to v; N with n for n; N with n that; N with n to v').split('; ')\n",
    "pgNoun += pgNoun + ['N %s -ing' % prep for prep in pgPreps ]\n",
    "pgNoun += pgNoun + ['ADJ %s n' % prep for prep in pgPreps if prep != 'of']+ ['N %s -ing' % prep for prep in pgPreps]\n",
    "pgAdj = ('ADJ adj; ADJ and adj; ADJ as to wh; '+\\\n",
    "        'ADJ enough; ADJ enough for n; ADJ enough for n to v; ADJ enough n; '+\\\n",
    "        'ADJ enough n for n; ADJ enough n for n to v; ADJ enough n that; ADJ enough to v; '+\\\n",
    "        'ADJ for n to v; ADJ from n to n; ADJ in color; ADJ -ing; '+\\\n",
    "        'ADJ in n as n; ADJ in n from n; ADJ in n to n; ADJ in n with n; ADJ in n as n; ADJ n for n'+\\\n",
    "        'ADJ n to v; ADJ on n for n; ADJ on n to v; ADJ that; ADJ to v; ADJ to n for n; ADJ n for -ing'+\\\n",
    "        'ADJ wh; ADJ on n for n; ADJ on n to v; ADJ that; ADJ to v; ADJ to n for n; ADJ n for -ing').split('; ')\n",
    "pgAdj += [ 'ADJ %s n'%prep for prep in pgPreps ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgPatterns = verbpat + pgAdj + pgNoun\n",
    "# pgPatterns = pgNoun\n",
    "\"\"\"\n",
    " 'N into -ing',\n",
    " 'N of -ing',\n",
    " 'N on -ing',\n",
    " 'N upon -ing',\n",
    " 'N over -ing',\n",
    " 'N through -ing'\n",
    "\"\"\"\n",
    "mapRW = dict( [ pair.split() for pair in reservedWords ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isverbpat(pat):\n",
    "#     return  pat in verbpat\n",
    "    return pat in pgPatterns\n",
    "\n",
    "def sentence_to_ngram(words, lemmas, tags, chunks): \n",
    "    return [ (k, k+degree) for k in range(1,len(words)) \\\n",
    "            for degree in range(2, min(maxDegree, len(words)-k+1)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapRest = dict( [('VBG', 'ing'), ('VBD', 'v-ed'), ('VBN', 'v-ed'), ('VB', 'v'),('NN', 'n'), ('NNS', 'n'), ('JJ', 'adj'), ('RB', 'adv'),('NP', 'n'),('VP','v'),('JP','adj'),('ADJP','adj'),('ADVP','adv'),('SBAR','that')] )\n",
    "mapHead = dict( [('H-NP', 'N'), ('H-VP', 'V'), ('H-ADJP', 'ADJ'), ('H-ADVP', 'ADV'), ('H-VB', 'V')] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTwoObjs(tag, chunk):\n",
    "    if chunk[-1] != 'H-NP': return False\n",
    "    return (len(tag) > 1 and tag[0] in pronOBJ) or (len(tag) > 1 and 'DT' in tag[1:])\n",
    "    \n",
    "def chunk_to_element(words, lemmas, tags, chunks, i, isHead):\n",
    "    #print ('***', i, words[i], lemmas[i], tags[i], chunks[i], isHead, tags[i][-1] == 'RP' and tags[i-1][-1][:2] == 'VB')\n",
    "    if isHead:\n",
    "#         x = mapHead[chunks[i][-1]] if chunks[i][-1] in mapHead else '*'\n",
    "#         print(chunks[i][-1],x)\n",
    "        return mapHead[chunks[i][-1]] if chunks[i][-1] in mapHead else '*'\n",
    "    if lemmas[i][0] == 'favour' and words[i-1][-1]=='in' and words[i+1][0]=='of': return 'favour'\n",
    "    if tags[i][-1] == 'RP' and tags[i-1][-1][:2] == 'VB':                return '_'\n",
    "    if tags[i][0][0]=='W' and lemmas[i][-1] in mapRW:                    return mapRW[lemmas[i][-1]]\n",
    "    if hasTwoObjs(tags[i], chunks[i]):                                              return 'n n'\n",
    "    if tags[i][-1] in mapRest:                            return mapRest[tags[i][-1]]\n",
    "    if tags[i][-1][:2] in mapRest:                        return mapRest[tags[i][-1][:2]]\n",
    "    if chunks[i][-1] in mapHead:\n",
    "#         print(mapHead[chunks[i][-1]].lower())\n",
    "        return mapHead[chunks[i][-1]].lower()\n",
    "    if lemmas[i][-1] in pgPreps:                                         return lemmas[i][-1]\n",
    "    return lemmas[i][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplifyPat(pat): return 'V' if pat == 'V ,' else pat.replace(' _', '').replace('_', ' ').replace('  ', ' ')\n",
    "    \n",
    "def ngram_to_pat(words, lemmas, tags, chunks, start, end):\n",
    "    pat, doneHead = [], False\n",
    "    for i in range(start, end):\n",
    "#         if tags[i][-1][0] in 'V ADJ N' and not doneHead:\n",
    "#             isHead = tags[i][-1][0]\n",
    "#         isHead = tags[i][-1][0] in ['V', 'N','J'] and not doneHead\n",
    "        isHead = tags[i][-1][0] in ['V', 'N','J'] and not doneHead\n",
    "        pat.append( chunk_to_element(words, lemmas, tags, chunks, i, isHead) )\n",
    "        if isHead: doneHead = True\n",
    "    pat = simplifyPat(' '.join(pat))\n",
    "    #print ('===', start, end, pat, words[start:end])\n",
    "    return pat if isverbpat(pat) else ''\n",
    "\n",
    "def ngram_to_head(words, lemmas, tags, chunks, start, end):\n",
    "    for i in range(start, end):\n",
    "        if tags[i][-1][0] in 'V' and tags[i+1][-1]=='RP':  return lemmas[i][-1].upper()+ ('_'+lemmas[i+1][-1].upper())\n",
    "        if tags[i][-1][0] in ['V', 'N','J']:  return lemmas[i][-1].upper()\n",
    "#         if tags[i][-1][0] in 'V':  return lemmas[i][-1].upper()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "akl = dict( [ (x+'-n', True) for x in 'focus, ability, absence, account, achievement, act, action, activity, addition, adoption, adult, advance, advantage, advice, age, aim, alternative, amount, analogy, analysis, application, approach, argument, aspect, assertion, assessment, assistance, association, assumption, attempt, attention, attitude, author, awareness, balance, basis, behaviour, behavior, being, belief, benefit, bias, birth, capacity, case, category, cause, centre, challenge, change, character, characteristic, choice, circumstance, class, classification, code, colleague, combination, commitment, committee, communication, community, comparison, complexity, compromise, concentration, concept, conception, concern, conclusion, condition, conduct, conflict, consensus, consequence, consideration, constraint, construction, content, contradiction, contrast, contribution, control, convention, correlation, country, creation, crisis, criterion, criticism, culture, damage, data, debate, decision, decline, defence, defense, definition, degree, demand, description, destruction, determination, development, difference, difficulty, dilemma, dimension, disadvantage, discovery, discrimination, discussion, distinction, diversity, division, doctrine, effect, effectiveness, element, emphasis, environment, error, essence, establishment, evaluation, event, evidence, evolution, examination, example, exception, exclusion, existence, expansion, experience, experiment, explanation, exposure, extent, extreme, fact, factor, failure, feature, female, figure, finding, force, form, formation, function, future, gain, group, growth, guidance, guideline, hypothesis, idea, identity, impact, implication, importance, improvement, increase, indication, individual, influence, information, insight, instance, institution, integration, interaction, interest, interpretation, intervention, introduction, investigation, isolation, issue, kind, knowledge, lack, learning, level, likelihood, limit, limitation, link, list, literature, logic, loss, maintenance, majority, male, manipulation, mankind, material, means, measure, medium, member, method, minority, mode, model, motivation, movement, need, network, norm, notion, number, observation, observer, occurrence, operation, opportunity, option, organisation, organization, outcome, output, paper, parallel, parent, part, participant, past, pattern, percentage, perception, period, person, personality, perspective, phenomenon, point, policy, population, position, possibility, potential, practice, presence, pressure, problem, procedure, process, production, programme, program, progress, property, proportion, proposition, protection, provision, publication, purpose, quality, question, range, rate, reader, reality, reason, reasoning, recognition, reduction, reference, relation, relationship, relevance, report, representative, reproduction, requirement, research, resistance, resolution, resource, respect, restriction, result, review, rise, risk, role, rule, sample, scale, scheme, scope, search, section, selection, sense, separation, series, service, set, sex, shift, significance, similarity, situation, skill, society, solution, source, space, spread, standard, statistics, stimulus, strategy, stress, structure, subject, success, summary, support, survey, system, target, task, team, technique, tendency, tension, term, theme, theory, tolerance, topic, tradition, transition, trend, type, uncertainty, understanding, unit, use, validity, value, variation, variety, version, view, viewpoint, volume, whole, work, world'.split(', ') ]+\\\n",
    "            [ (x+'-v', True)  for x in 'accept, account, achieve, acquire, act, adapt, adopt, advance, advocate, affect, aid, aim, allocate, allow, alter, analyse, analyze, appear, apply, argue, arise, assert, assess, assign, associate, assist, assume, attain, attempt, attend, attribute, avoid, base, be, become, benefit, can, cause, characterise, characterize, choose, cite, claim, clarify, classify, coincide, combine, compare, compete, comprise, concentrate, concern, conclude, conduct, confine, conform, connect, consider, consist, constitute, construct, contain, contrast, contribute, control, convert, correspond, create, damage, deal, decline, define, demonstrate, depend, derive, describe, design, destroy, determine, develop, differ, differentiate, diminish, direct, discuss, display, distinguish, divide, dominate, effect, eliminate, emerge, emphasize, employ, enable, encounter, encourage, enhance, ensure, establish, evaluate, evolve, examine, exceed, exclude, exemplify, exist, expand, experience, explain, expose, express, extend, facilitate, fail, favour, favor, finance, focus, follow, form, formulate, function, gain, generate, govern, highlight, identify, illustrate, imply, impose, improve, include, incorporate, increase, indicate, induce, influence, initiate, integrate, interpret, introduce, investigate, involve, isolate, label, lack, lead, limit, link, locate, maintain, may, measure, neglect, note, obtain, occur, operate, outline, overcome, participate, perceive, perform, permit, pose, possess, precede, predict, present, preserve, prevent, produce, promote, propose, prove, provide, publish, pursue, quote, receive, record, reduce, refer, reflect, regard, regulate, reinforce, reject, relate, rely, remain, remove, render, replace, report, represent, reproduce, require, resolve, respond, restrict, result, retain, reveal, seek, select, separate, should, show, solve, specify, state, stimulate, strengthen, stress, study, submit, suffer, suggest, summarise, summarize, supply, support, sustain, tackle, tend, term, transform, treat, undermine, undertake, use, vary, view, write, yield'.split(', ') ]+\\\n",
    "            [ (x+'-adj', True) for x in 'absolute, abstract, acceptable, accessible, active, actual, acute, additional, adequate, alternative, apparent, applicable, appropriate, arbitrary, available, average, basic, central, certain, clear, common, competitive, complete, complex, comprehensive, considerable, consistent, conventional, correct, critical, crucial, dependent, detailed, different, difficult, distinct, dominant, early, effective, equal, equivalent, essential, evident, excessive, experimental, explicit, extensive, extreme, far, favourable, favorable, final, fixed, following, formal, frequent, fundamental, future, general, great, high, human, ideal, identical, immediate, important, inadequate, incomplete, independent, indirect, individual, inferior, influential, inherent, initial, interesting, internal, large, late, leading, likely, limited, local, logical, main, major, male, maximum, mental, minimal, minor, misleading, modern, mutual, natural, necessary, negative, new, normal, obvious, original, other, overall, parallel, partial, particular, passive, past, permanent, physical, positive, possible, potential, practical, present, previous, primary, prime, principal, productive, profound, progressive, prominent, psychological, radical, random, rapid, rational, real, realistic, recent, related, relative, relevant, representative, responsible, restricted, scientific, secondary, selective, separate, severe, sexual, significant, similar, simple, single, so-called, social, special, specific, stable, standard, strict, subsequent, substantial, successful, successive, sufficient, suitable, surprising, symbolic, systematic, theoretical, total, traditional, true, typical, unique, unlike, unlikely, unsuccessful, useful, valid, valuable, varied, various, visual, vital, wide, widespread'.split(', ') ]+\\\n",
    "            [ (x+'-adv', True) for x in 'above, accordingly, accurately, adequately, also, approximately, at best, basically, clearly, closely, commonly, consequently, considerably, conversely, correctly, directly, effectively, e.g., either, equally, especially, essentially, explicitly, extremely, fairly, far, for example, for instance, frequently, fully, further, generally, greatly, hence, highly, however, increasingly, indeed, independently, indirectly, inevitably, initially, in general, in particular, largely, less, mainly, more, moreover, most, namely, necessarily, normally, notably, often, only, originally, over, partially, particularly, potentially, previously, primarily, purely, readily, recently, relatively, secondly, significantly, similarly, simply, socially, solely somewhat, specifically, strongly, subsequently, successfully, thereby, therefore, thus, traditionally, typically, ultimately, virtually, wholly, widely'.split(', ') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# sss = 'FOCUS-N'\n",
    "# print(akl[sss.lower()])\n",
    "# # akl['focus-n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all the commune members , young and old , went out to hervest the crops .\n",
      "GO_OUT\tV\twent out\t9\t10\n",
      "GO_OUT\tV to v\twent out to hervest\t9\t12\n",
      "HERVEST\tV n\thervest the crops\t12\t14\n"
     ]
    }
   ],
   "source": [
    "# line = [('all the commune members', ',', 'young and old', ',', 'went', 'out', 'to', 'hervest', 'the crops', '.'), ('all the commune member', ',', 'young and old', ',', 'go', 'out', 'to', 'hervest', 'the crop', '.'), ('PDT DT JJ NNS', ',', 'JJ CC JJ', ',', 'VBD', 'RP', 'TO', 'VB', 'DT NNS', '.'), ('I-NP I-NP I-NP H-NP', 'O', 'I-NP I-NP H-NP', 'O', 'H-VP', 'H-PRT', 'H-TO', 'H-VP', 'I-NP H-NP', 'O')]\n",
    "# line = str(line)\n",
    "\n",
    "# parse = eval(line.strip())\n",
    "# parse = [ [y.split() for y in x]  for x in parse ]\n",
    "# print ('\\n'+' '.join([' '.join(x) for x in parse[0] ]))\n",
    "# to_token = ' '.join([' '.join(x) for x in parse[0] ]).split(' ')\n",
    "# # print(to_token)\n",
    "\n",
    "# for start, end in sentence_to_ngram(*parse):\n",
    "\n",
    "# # for n, start, end in enumerate(sentence_to_ngram(*parse)):\n",
    "#     pat = ngram_to_pat(*parse, start, end)\n",
    "#     if pat:\n",
    "#         word = ngram_to_head(*parse, start, end)\n",
    "#         temp_token = ' '.join([' '.join(x) for x in parse[0][start:end]]).split(' ')\n",
    "#         len_token = len(temp_token)\n",
    "# #         print(len_token)\n",
    "#         for n, t in enumerate(to_token):\n",
    "# #             print(temp_token)\n",
    "# #             print(to_token[n:n+len_token+1])\n",
    "#             if temp_token == to_token[n:n+len_token]:\n",
    "#                 token_start = n\n",
    "#                 token_end = n+len_token-1\n",
    "# #                 print(n,token_start,token_end)\n",
    "# #         for i in range(start,end+1):\n",
    "# #             print(i)\n",
    "        \n",
    "#                 print('%s\\t%s\\t%s\\t%d\\t%d' %(word,pat,' '.join([' '.join(x) for x in parse[0][start:end]]),token_start,token_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for line in open('UM-Corpus.en.200k.tagged.txt'):\n",
    "\n",
    "#     for line in sys.stdin:\n",
    "        parse = eval(line.strip())\n",
    "        parse = [ [y.split() for y in x]  for x in parse ]\n",
    "        print ('\\n'+' '.join([' '.join(x) for x in parse[0] ]))\n",
    "        \n",
    "        to_token = ' '.join([' '.join(x) for x in parse[0] ]).split(' ')\n",
    "# print(to_token)\n",
    "\n",
    "        for start, end in sentence_to_ngram(*parse):\n",
    "\n",
    "# for n, start, end in enumerate(sentence_to_ngram(*parse)):\n",
    "            pat = ngram_to_pat(*parse, start, end)\n",
    "            if pat:\n",
    "                word = ngram_to_head(*parse, start, end)\n",
    "                temp_token = ' '.join([' '.join(x) for x in parse[0][start:end]]).split(' ')\n",
    "                len_token = len(temp_token)\n",
    "#         print(len_token)\n",
    "                for n, t in enumerate(to_token):\n",
    "#             print(temp_token)\n",
    "#             print(to_token[n:n+len_token+1])\n",
    "                    if temp_token == to_token[n:n+len_token]:\n",
    "                        token_start = n\n",
    "                        token_end = n+len_token-1\n",
    "#                 print(n,token_start,token_end)\n",
    "#         for i in range(start,end+1):\n",
    "#             print(i)\n",
    "        \n",
    "                        print('%s\\t%s\\t%s\\t%d\\t%d' %(word,pat,' '.join([' '.join(x) for x in parse[0][start:end]]),token_start,token_end))\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = [('all that', 'remains', 'for', 'me', 'to', 'do is', 'to', 'say', 'good-bye', '.'), ('all that', 'remain', 'for', 'me', 'to', 'do be', 'to', 'say', 'good-bye', '.'), ('PDT DT', 'VBZ', 'IN', 'PRP', 'TO', 'VB VBZ', 'TO', 'VB', 'NN', '.'), ('I-NP H-NP', 'H-VP', 'H-PP', 'H-NP', 'H-TO', 'I-VP H-VB', 'H-TO', 'H-VP', 'H-NP', 'O')]\n",
    "# line = str(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all that remains for me to do is to say good-bye .\n",
      "REMAIN-V\tV for n\t\n",
      "all that remains for me to do is to say good-bye .\n",
      "BE-V\tV to v\t\n",
      "all that remains for me to do is to say good-bye .\n"
     ]
    }
   ],
   "source": [
    "# parse = eval(line.strip())\n",
    "# parse = [ [y.split() for y in x]  for x in parse ]\n",
    "# print ('\\n'+' '.join([' '.join(x) for x in parse[0] ]))\n",
    "\n",
    "\n",
    "\n",
    "# for start, end in sentence_to_ngram(*parse):\n",
    "\n",
    "#     pat = ngram_to_pat(*parse, start, end)\n",
    "#     if pat:\n",
    "#         word = ngram_to_head(*parse, start, end)\n",
    "#         tag = pat.split(' ')[0]\n",
    "#         w_tag = word+'-'+tag\n",
    "#         if w_tag.lower() in akl:\n",
    "#             print('%s\\t%s\\t%s' %(w_tag,pat,'\\n'+' '.join([' '.join(x) for x in parse[0] ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
