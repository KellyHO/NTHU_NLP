{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk, random\n",
    "from nltk.probability import DictionaryProbDist as D\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "import nltk, random\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leskOverlap(senseDef, target):\n",
    "    wnidCount = [ (wncat, tf, word, len(DF[word])+1) for word in senseDef \\\n",
    "                                                    for wncat, tf in TF[word].items() \\\n",
    "                                                    if wncat in target.values() ]\n",
    "    res = sorted( [ (word, tf*int(1000/df)) for wnid, tf, word, df in wnidCount], key = lambda x: -x[1])[:5]\n",
    "    feature = {}\n",
    "#         print(res)\n",
    "    for i in res:\n",
    "        feature[i[0]] = i[1]\n",
    "    print(feature)\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "TF = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "DF = defaultdict(lambda: [])\n",
    "\n",
    "def wnTag(pos): return {'noun': 'n', 'verb': 'v', 'adjective': 'a', 'adverb': 'r'}[pos]\n",
    "\n",
    "def trainLesk():\n",
    "    training_x = []\n",
    "    training_y = []\n",
    "    features = []\n",
    "    training = [  line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '' ]\n",
    "    training = training[:100]\n",
    "#     training, testing = train_test_split(training,test_size=0.2)\n",
    "#     print(training[0])\n",
    "    def isHead(head, word, tag):\n",
    "        try:\n",
    "            return lmtzr.lemmatize(word, tag) == head\n",
    "        except:\n",
    "            return False\n",
    "            \n",
    "    # zucchini-n-2\tvegetable.n.01\tzucchini courgette||small cucumber-shaped vegetable marrow; typically dark green||\t\n",
    "    # {'zucchini-n-1': 'vine.n.01', 'zucchini-n-2': 'vegetable.n.01'}        \n",
    "    for wnid, wncat, senseDef, target in training:\n",
    "#         print(senseDef)\n",
    "        Tf = {}\n",
    "        head, pos = wnid.split('-')[:2]\n",
    "        for word in words(senseDef):\n",
    "            Tf[word] = True\n",
    "#         print(Tf)\n",
    "#         training_x.append(Tf)\n",
    "#         training_y.append(wncat)\n",
    "        \n",
    "            if word != head and not isHead(head, word, pos):\n",
    "#                 f = gender_features(word)\n",
    "#                 print(type(f),f)\n",
    "#                 Tf.append(f)\n",
    "#         training_data.append(Tf)\n",
    "#         training_label.append(wncat)\n",
    "#                 print(word)\n",
    "                TF[word][wncat] += 1\n",
    "                DF[word] += [] if wncat in DF[word] else [wncat]\n",
    "#                 print(TF[word][wncat])\n",
    "#             features.append((Tf,wncat))\n",
    "            \n",
    "\n",
    "    for evpid, wncat, senseDef, target in training:\n",
    "        features.append((leskOverlap(words(senseDef), eval(target)),wncat))\n",
    "        #head, pos = evpid.split('-')[:2]\n",
    "#         print( '%s\\t%s\\t%s' %(evpid, leskOverlap(words(senseDef), eval(target)), senseDef.split('||')[0] ) )\n",
    "#             Tf[word] = True\n",
    "    \n",
    "            \n",
    "#     print(DF[\"abandon\"][\"unconcerned.a.01\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'empty': 666, 'leave': 500, 'parking': 500, 'lot': 500, 'forsake': 333}\n",
      "{'intent': 500, 'claiming': 500, 'again': 500, 'life': 500, 'god': 500}\n",
      "{'vacate': 1000, 'empty': 666, 'leave': 500}\n",
      "{'claims': 1000, 'maintaining': 500, 'insisting': 500, 'ideas': 500}\n",
      "{'desolate': 500, 'desert': 500, 'leave': 500, 'counts': 500}\n",
      "{'derelict': 500, 'forsaken': 500, 'owner': 500, 'inhabitants': 500, 'weed': 500}\n",
      "{'free': 500, 'constraint': 500, 'sadness': 500, 'born': 500, 'grief': 500}\n",
      "{'quality': 666, 'able': 500, 'perform': 500, 'permits': 500}\n",
      "{'qualities': 1000, 'mental': 1000, 'power': 500, 'possession': 500}\n",
      "{'unnatural': 500, 'typical': 500, 'regular': 500, 'conforming': 500, 'norm': 500}\n",
      "{'departing': 500, 'e': 500, 'g': 500, 'development': 500, 'they': 500}\n",
      "{'much': 500, 'greater': 500, 'than': 500, 'profits': 500, 'ambition': 500}\n",
      "{'ship': 1000, 'on_board': 500, 'train': 500, 'plane': 500, 'other': 500}\n",
      "{'second': 1000, 'on_base': 500, 'first': 500, 'third': 500}\n",
      "{'side': 1000, 'close': 1000, 'ship': 1000, 'alongside': 500}\n",
      "{'been': 666, 'years': 666, 's': 625, 'part': 500, 'bill': 500}\n",
      "{'get_rid_of': 500, 'slavery': 500, 'abolished': 500, 'mid': 500, '19th': 500}\n",
      "{'termination': 500, 'pregnancy': 500, 'of': 26}\n",
      "{'miscarriage': 500, 'plan': 500, 'failure': 333, 'of': 78, 'a': 40}\n",
      "{'around': 7500, 'all': 1332, 'approximately': 1000, 'roughly': 1000}\n",
      "{'around': 7500}\n",
      "{'around': 7500, 'vicinity': 500, 'few': 500}\n",
      "{'around': 7500}\n",
      "{'astir': 1000, 'whole': 500, 'town': 500, 'over': 500}\n",
      "{'place': 1998, 'supra': 1000, 'earlier': 1000, 'see': 1000, 'at': 600}\n",
      "{'place': 1998, 'higher_up': 1000, 'in_a_higher_place': 1000, 'to_a_higher_place': 1000, 'higher': 1000}\n",
      "{'place': 1998, 'supra': 1000, 'earlier': 1000, 'see': 1000, 'at': 600}\n",
      "{'place': 1998, 'higher_up': 1000, 'in_a_higher_place': 1000, 'to_a_higher_place': 1000, 'higher': 1000}\n",
      "{'had': 666, 'foreign': 500, 'travelled': 500, 'a': 440, 'or': 375}\n",
      "{'afield': 1000, 's': 625, 'far': 500, 'home': 500}\n",
      "{'place': 1998, 'overseas': 500, 'across': 500, 'ocean': 500, 'a': 440}\n",
      "{'sharp': 666, 'disconnected': 500, 'changes': 500, 'transitions': 500, 'prose': 500}\n",
      "{'exceedingly': 500, 'unexpected': 500, 'weather': 500, 'sudden': 333}\n",
      "{'precipitous': 1500, 'sharp': 666}\n",
      "{'surprisingly': 500, 'unceremoniously': 500, 'brusque': 500, 'manner': 333, 'reply': 333}\n",
      "{'suddenly': 1000, 'short': 500, 'warning': 500, 'stopped': 500}\n",
      "{'absent': 500, 'surprised': 500, 'any': 500, 'explanation': 500, 'state': 250}\n",
      "{'failure': 333, 'present': 333, 'be': 125, 'to': 33}\n",
      "{'during': 1000, 'interval': 500, 'somebody': 500, 'visited': 500}\n",
      "{'absence_seizure': 500, 'occurrence': 500, 'abrupt': 500, 'transient': 500, 'loss': 500}\n",
      "{'specified': 500, 'place': 333, 'being': 200, 'not': 100, 'in': 40}\n",
      "{'lacking': 1000, 'missing': 500, 'wanting': 500, 'nonexistent': 500}\n",
      "{'absentminded': 1000, 'scatty': 1000, 'abstracted': 500}\n",
      "{'perfect': 500, 'pure': 500, 'loyalty': 500, 'silence': 500, 'complete': 333}\n",
      "{'out': 1998}\n",
      "{'limited': 500, 'monarch': 500, 'law': 333, 'an': 141, 'not': 100}\n",
      "{'finality': 500, 'implication': 500, 'possible': 500, 'guarantee': 500, 'respect': 500}\n",
      "{'perfectly': 1500, 'utterly': 1000, 'dead': 1000}\n",
      "{'lie': 666, 'totally': 500, 'definitely': 500, 'opposed': 500, 'forced': 500}\n",
      "{'become': 1000, 'imbued': 500, 'liquids': 500, 'gases': 500, 'light': 250}\n",
      "{'ingest': 500, 'mentally': 500, 'knowledge': 500, 'beliefs': 500, 'tribe': 500}\n",
      "{'payments': 500, 'costs': 500, 'take_over': 333, 'debts': 333, 'up': 142}\n",
      "{'imbibe': 500, 'sop_up': 500, 'suck_up': 500, 'draw': 500, 'take_up': 500}\n",
      "{'become': 1000, 'tax': 1000, 'sales': 500, 'income': 500}\n",
      "{'black': 500, 'star': 500, 'matter': 500, 'suck': 333}\n",
      "{'immerse': 500, 'engulf': 500, 'plunge': 500, 'engross': 500, 'devote': 500}\n",
      "{'assimilate': 333, 'immigrants': 333, 'quickly': 333, 'society': 333}\n",
      "{'existing': 500, 'mind': 500, 'separated': 500, 'embodiment': 500, 'like': 500}\n",
      "{'abstractionist': 500, 'nonfigurative': 500, 'nonobjective': 500, 'representing': 500, 'imitating': 500}\n",
      "{'practical': 1000, 'dealing': 500, 'intention': 500, 'reasoning': 500, 'science': 500}\n",
      "{'inconsistent': 500, 'reason': 500, 'logic': 500, 'sense': 500, 'predicament': 500}\n",
      "{'nonsensical': 1500, 'cockeyed': 1000, 'laughable': 1000}\n",
      "{'ill': 1000, 'treatment': 1000, 'cruel': 500}\n",
      "{'revilement': 500, 'contumely': 500, 'vilification': 500, 'rude': 500, 'intended': 500}\n",
      "{'misuse': 500, 'improper': 500, 'excessive': 500, 'public': 500, 'funds': 500}\n",
      "{'opprobrious': 500, 'scurrilous': 500, 'offensive': 500, 'reproach': 500, 'expressing': 333}\n",
      "{'characterized': 500, 'psychological': 500, 'punishment': 500, 'argued': 500, 'foster': 500}\n",
      "{'associated': 500, 'academia': 500, 'academy': 500, 'curriculum': 500, 'gowns': 500}\n",
      "{'practical': 1000, 'hypothetical': 500, 'theoretical': 500, 'expected': 500, 'produce': 500}\n",
      "{'donnish': 500, 'pedantic': 500, 'narrow': 500, 'focus': 500, 'display': 500}\n",
      "{'quicken': 500, 'speed_up': 333, 'speed': 333}\n",
      "{'speed': 333, 'speed_up': 333, 'cause': 333}\n",
      "{'speech_pattern': 500, 'oral': 500, 'couldn': 500, 'suppress': 500, 'contemptuous': 500}\n",
      "{'red': 1000, 'emphasis': 666, 'importance': 500}\n",
      "{'dialect': 1500, 'has': 666}\n",
      "{'syllable': 1000, 'stress': 999}\n",
      "{'indicate': 1000, 'stress': 999, 'accent_mark': 500, 'diacritical': 500}\n",
      "{'i': 664, 'dogma': 500, 'church': 500, 'argument': 500, 'consider': 333}\n",
      "{'have': 1000, 'receive': 1000, 'i': 664}\n",
      "{'favorably': 1000, 'i': 664, 'go_for': 500, 'affirmative': 500}\n",
      "{'favorably': 1000, 'react': 500, 'proper': 500, 'did': 500, 'atonal': 500}\n",
      "{'admit': 1500, 'have': 1000, 'take_on': 500}\n",
      "{'bear': 1000, 'i': 664, 'assume': 500, 'own': 500}\n",
      "{'have': 1000, 'i': 664, 'live_with': 500, 'swallow': 500}\n",
      "{'designed': 500, 'surface': 500, 'will': 500, 'dye': 500, 'hold': 333}\n",
      "{'receive': 1000, 'report': 500, 'officially': 500, 'committee': 500, 'a': 160}\n",
      "{'risk': 500, 'opportunity': 500, 'make': 333, 'take': 270}\n",
      "{'satisfactory': 666, 'worthy': 500, 'acceptance': 500, 'levels': 500, 'radiation': 500}\n",
      "{'judged': 500, 'conformity': 500, 'usage': 500, 'approved': 333}\n",
      "{'satisfactory': 666, 'meeting': 500, 'requirements': 500, 'step': 500}\n",
      "{'adequate': 500, 'drinking': 500, 'water': 333, 'purpose': 250, 'for': 180}\n",
      "{'credence': 1000, 'mental': 1000, 'attitude': 500, 'believable': 500}\n",
      "{'adoption': 1000, 'acceptation': 500, 'espousal': 500, 'act': 500}\n",
      "{'acceptable': 500, 'torn': 500, 'jeans': 500, 'received': 500, 'club': 500}\n",
      "{'contract': 1000, 'signifying': 500, 'terms': 500, 'offer': 500}\n",
      "{'recognized': 1000, 'recognised': 500, 'generally': 500, 'compelling': 500}\n",
      "{'entree': 500, 'accession': 500, 'admission': 500, 'admittance': 500, 'right': 500}\n",
      "{'right': 500, 'obtain': 500, 'advantage': 500, 'services': 500, 'membership': 500}\n",
      "{'approach': 500, 'entering': 500, 'leaving': 500, 'took': 500, 'bridge': 500}\n",
      "{'access_code': 500, 'code': 500, 'series': 500, 'characters': 500, 'digits': 500}\n"
     ]
    }
   ],
   "source": [
    "features = trainLesk()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'abandon': True, 'forsake': True, 'leave': True, 'behind': True, 'we': True, 'abandoned': True, 'the': True, 'old': True, 'car': True, 'in': True, 'empty': True, 'parking': True, 'lot': True}, 'get_rid_of.v.01')\n"
     ]
    }
   ],
   "source": [
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LG_gender(train_set, test_set):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    from nltk.classify import SklearnClassifier \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)\n",
    "#     print(sklearn_classifier.prob_classify(gender_features('mark'))._prob_dict)\n",
    "    print(nltk.classify.accuracy(sklearn_classifier, test_set))\n",
    "    return sklearn_classifier\n",
    "\n",
    "def ME_gender(train_set, test_set):\n",
    "    print('== NLTK MaxEnt ==')\n",
    "    from nltk.classify import MaxentClassifier\n",
    "    nltk_classifier = MaxentClassifier.train(train_set, nltk.classify.MaxentClassifier.ALGORITHMS[0])\n",
    "#     print(nltk_classifier.prob_classify(gender_features('mark'))._prob_dict)\n",
    "    print(nltk.classify.accuracy(nltk_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(features)*9//10\n",
    "train_set, test_set = features[:split_point], features[split_point:]\n",
    "classifier = LG_gender(train_set, test_set)\n",
    "# ME_gender(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
