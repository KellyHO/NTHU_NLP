{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk, random\n",
    "from nltk.probability import DictionaryProbDist as D\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "import nltk, random\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leskOverlap(senseDef, target):\n",
    "    wnidCount = [ (wncat, tf, word, len(DF[word])+1) for word in senseDef \\\n",
    "                                                    for wncat, tf in TF[word].items() \\\n",
    "                                                    if wncat in target.values() ]\n",
    "    res = sorted( [ (word, tf*int(1000/df)) for wnid, tf, word, df in wnidCount], key = lambda x: -x[1])[:5]\n",
    "    feature = {}\n",
    "#         print(res)\n",
    "    for i in res:\n",
    "        feature[i[0]] = i[1]\n",
    "#     print(feature)\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "TF = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "DF = defaultdict(lambda: [])\n",
    "\n",
    "def wnTag(pos): return {'noun': 'n', 'verb': 'v', 'adjective': 'a', 'adverb': 'r'}[pos]\n",
    "\n",
    "def trainLesk():\n",
    "    training_x = []\n",
    "    training_y = []\n",
    "    features = []\n",
    "    training = [  line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '' ]\n",
    "#     training = training[:100]\n",
    "#     training, testing = train_test_split(training,test_size=0.2)\n",
    "#     print(training[0])\n",
    "    def isHead(head, word, tag):\n",
    "        try:\n",
    "            return lmtzr.lemmatize(word, tag) == head\n",
    "        except:\n",
    "            return False\n",
    "            \n",
    "    # zucchini-n-2\tvegetable.n.01\tzucchini courgette||small cucumber-shaped vegetable marrow; typically dark green||\t\n",
    "    # {'zucchini-n-1': 'vine.n.01', 'zucchini-n-2': 'vegetable.n.01'}        \n",
    "    for wnid, wncat, senseDef, target in training:\n",
    "#         print(senseDef)\n",
    "        Tf = {}\n",
    "        head, pos = wnid.split('-')[:2]\n",
    "        for word in words(senseDef):\n",
    "            Tf[word] = True\n",
    "#         print(Tf)\n",
    "#         training_x.append(Tf)\n",
    "#         training_y.append(wncat)\n",
    "        \n",
    "            if word != head and not isHead(head, word, pos):\n",
    "#                 f = gender_features(word)\n",
    "#                 print(type(f),f)\n",
    "#                 Tf.append(f)\n",
    "#         training_data.append(Tf)\n",
    "#         training_label.append(wncat)\n",
    "#                 print(word)\n",
    "                TF[word][wncat] += 1\n",
    "                DF[word] += [] if wncat in DF[word] else [wncat]\n",
    "#                 print(TF[word][wncat])\n",
    "#             features.append((Tf,wncat))\n",
    "            \n",
    "\n",
    "    for evpid, wncat, senseDef, target in training:\n",
    "        features.append((leskOverlap(words(senseDef), eval(target)),wncat))\n",
    "        #head, pos = evpid.split('-')[:2]\n",
    "#         print( '%s\\t%s\\t%s' %(evpid, leskOverlap(words(senseDef), eval(target)), senseDef.split('||')[0] ) )\n",
    "#             Tf[word] = True\n",
    "    \n",
    "            \n",
    "#     print(DF[\"abandon\"][\"unconcerned.a.01\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = trainLesk()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LG_gender(train_set, test_set):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    from nltk.classify import SklearnClassifier \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)\n",
    "#     print(sklearn_classifier.prob_classify(gender_features('mark'))._prob_dict)\n",
    "    print(nltk.classify.accuracy(sklearn_classifier, test_set))\n",
    "    return sklearn_classifier\n",
    "\n",
    "def ME_gender(train_set, test_set):\n",
    "    print('== NLTK MaxEnt ==')\n",
    "    from nltk.classify import MaxentClassifier\n",
    "    nltk_classifier = MaxentClassifier.train(train_set, nltk.classify.MaxentClassifier.ALGORITHMS[0])\n",
    "#     print(nltk_classifier.prob_classify(gender_features('mark'))._prob_dict)\n",
    "    print(nltk.classify.accuracy(nltk_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(features)*9//10\n",
    "train_set, test_set = features[:split_point], features[split_point:]\n",
    "classifier = LG_gender(train_set, test_set)\n",
    "# ME_gender(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
